{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project. \n",
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like mtcars, iris, palmer penguins etc.\n",
    "- The dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training a supervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your ML system.\n",
    "- You will evaluate the performance of your ML system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n",
    "\n",
    "\n",
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peer Review\n",
    "\n",
    "You will all have an opportunity to look at the Project Proposals of other groups to fuel your creativity and get more ideas for how you can improve your own projects. \n",
    "\n",
    "Both the project proposal and project checkpoint will have peer review."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Peter Barnett\n",
    "- William Lutz\n",
    "- Ricardo Sedano"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Our goal is to predict the minimum number of character substitutions required to turn a given input string of alphabetical characters into an English dictionary word (into *any* English dictionary word). That is, to predict the hamming distance between a given input string and its nearest English word. This problem can be solved with brute force nearest-neighbor search, or with a BK-tree, but takes a considerable amount of time if one needs to make this prediction many times. So the purpose of this project is to be able to generate approximations of this shortest distance more quickly. The words will come from the open source WordSet dictionary, the training inputs will be generated from random strings, and the training labels will be generated using the brute force search approach. The same process is used to create the test data. We will train our model on this data and its performance will be measured by computing the average error |predicted distance - true distance| across the test dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background,\n",
    "\n",
    "Hamming distance is the measured difference between two strings of equal lengths. As a simple example, the two strings 'car' and 'cat' would have a Hamming distance of 1, meaning that they only differ by one character. This mathematical concept was first published in 1950 his paper 'Error detecting error correcting codes' <a name = \"hamming\"></a>[<sup>[1]</sup>](#hammingnote). \n",
    "</br>\n",
    "</br>\n",
    "While it has widely been used in error detection, DNA sequencing, image processing, and NLP, trying to find the minimum Hamming distance between an input and a large dataset is computationally expensive to calculate when iterating through many examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "You should have a strong idea of what dataset(s) will be used to accomplish this project. \n",
    "\n",
    "If you know what (some) of the data you will use, please give the following information for each dataset:\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc will be needed\n",
    "\n",
    "If you don't yet know what your dataset(s) will be, you should describe what you desire in terms of the above bullets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "One solution to this problem is to use a multilayer perceptron with two hidden layers. The input layer consists of 27 x 10 = 270 nodes, representing the one-hot encoding of an alphabetical string with up to 10 letters (26 possible letters, plus 1 for the \"empty letter\" if the input word is less than 10 characters). The values in these nodes would be 1s if that letter is present in that position or 0s otherwise. The output layer would consist of one node, the value of which represents the predicted minimum hamming distance to the input word across all English dictionary words. The loss function would be the sum of squared distances between the predicted hamming distance and the actual hamming distance between the training set strings and their nearest English words. Since we can generate endless training data using the brute force search, we may not need to add a regularization term, but we may also add that. Then we use backpropogation to train the multilayer perception with that loss function, after splitting the training data up into batches for the loss function. The solution will be tested by measuring the average error rate |predicted distance - true distance| across the test set, and by measuring the average time the forward pass takes and comparing it to the average time a bk-tree takes to find the shortest distance with our dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "In order to evaluate the accuracy of our model we will split our dataset of randomly generated strings into a test and training dataset. For the test dataset, we will use tradition methods to calculate the Hamming distance for each string in the dataset. Our metric will be calculating the average error between the predicted Hamming distance and the actual value. Our model will be considered accurate if it is both able to identify the correct Hamming distance more than 75% of the time and it is computationally more efficient than traditional Hamming calculations on the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are little to no obvious ethics & privacy concerns that arise from our project. The process highlighted in the project computes a numerical value (hamming distance), which is not involving other data, especially excluding personal data. Seeing as this process takes a string and compares to an established word, we could see a potential algorithm that creates text dialogue from these words. This connects to artificial intellegence; AI may be able to generate converstations with users online, which could involve ethics issues in spontaneous text production or privacy issues if AI creates upon platforms asking for personal information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each project team member has agreed to and and is expected to: \n",
    "* Attend all-team meetings\n",
    "* Remain attentive; communicate quickly and effectively\n",
    "* Do work assigned to them\n",
    "* Be respectful of each others' work\n",
    "* Stay aware of deadlines and collaborate in favor of completing a comprehensive project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/22  |  6 PM |  Brainstorm topics (all); exchange contact information | Project topic, Discuss ideal dataset(s) and ethics;Edit, finalize, and submit project proposal| \n",
    "| 3/6  |  7 PM |  Import & Wrangle Data | Discuss Wrangling and possible analytical approaches; Finalize wrangling/EDA; Begin programming for project | \n",
    "| 3/20  | 7 PM  | Continue programming for project | Discuss/edit project code; Draft results/conclusion/discussion   |\n",
    "| 3/22  | Before 11:59 PM  | Discuss/edit full project; Complete project | Turn in Final Project  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"hammingnote\"></a>1.[^](#hamming): Hamming, R. W. (1950) Error detecting and error correcting codes. *Bell Systems Technical Journal, 29(2)*. https://ieeexplore.ieee.org/document/6772729<br> \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
