{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1dbeca9",
   "metadata": {},
   "source": [
    "# Alternate Algorithms to test against MLP NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcbc2c3",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fc283e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import string\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30802e0",
   "metadata": {},
   "source": [
    "## Loading in pre-tagged Hamming distance data calculated by brute force\n",
    "Note, only 2.5k datapoints are being loaded into this Jupyter file due to the excessive run times of each of the following models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f527a921",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5000\n",
    "MAX_WORD_LENGTH = 8\n",
    "\n",
    "ALPHABET = string.ascii_lowercase + '_'\n",
    "\n",
    "def one_hot_encode(strings, dists, device):\n",
    "    strings = strings.tolist()\n",
    "    dists = dists.tolist()\n",
    "    indices_np = np.array([[ord(c) - ord('a') for c in s] for s in strings], dtype=np.int64)\n",
    "    x_np = np.eye(len(ALPHABET), dtype=np.float32)[indices_np]\n",
    "    x = torch.from_numpy(x_np).to(device)\n",
    "    y = torch.as_tensor(dists, device=device)\n",
    "    x = x.numpy()\n",
    "    y = y.numpy()\n",
    "    return x.reshape(x.shape[0],x.shape[1]*x.shape[2]), y\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "fileName = 'mixed_data' + str(MAX_WORD_LENGTH) + '.csv'\n",
    "data = pd.read_csv(fileName, nrows=N, header=None, usecols=[0, 1], names=['strings', 'dists'])\n",
    "data['dists'] = data['dists'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936265a6",
   "metadata": {},
   "source": [
    "## Creating a single Random Forest with manually selected hyperparameters\n",
    "Set hyperparameters to the following values:\n",
    "   * n_estimators = 100\n",
    "   * max_depth = 20\n",
    "   * random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4b690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Model: 100%|██████████| 100/100 [05:01<00:00,  3.02s/iterations, Estimated remaining time =0m 27s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 1.07\n",
      "Classification accuracy: 0.39\n",
      "Runtime: 301.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2)\n",
    "train_strings, train_dists = train_data['strings'], train_data['dists']\n",
    "test_strings, test_dists = test_data['strings'], test_data['dists']\n",
    "train_data_x, train_data_y = one_hot_encode(train_strings, train_dists, device)\n",
    "test_data_x, test_data_y = one_hot_encode(test_strings, test_dists, device)\n",
    "\n",
    "# train random forest on train data\n",
    "model = RandomForestRegressor(n_estimators = 100, max_depth = 20, random_state = 42)\n",
    "#model.fit(train_data_x, train_data_y)\n",
    "\n",
    "# creating a progress bar\n",
    "pbar = tqdm(range(100), desc='Training Model', unit='iterations')\n",
    "\n",
    "for i in pbar:\n",
    "    model.fit(train_data_x, train_data_y)\n",
    "    if i % 10 == 0:\n",
    "        remaining_time = (time.time() - start_time) / (i + 1) * (100 - i - 1)\n",
    "        pbar.set_postfix({'Estimated remaining time ': f'{int(remaining_time // 60)}m {int(remaining_time % 60)}s'})\n",
    "        pbar.refresh()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# evaluate model on test data\n",
    "pred_y = model.predict(test_data_x)\n",
    "\n",
    "# initializing a new array to add the rounded Hamming distances\n",
    "rounded_pred_y = np.array([])\n",
    "\n",
    "for prediction in pred_y:\n",
    "    \n",
    "    # round the predicted values to the nearest Hamming dist.\n",
    "    rounded_pred_y = np.append(rounded_pred_y, int(np.round(prediction)))\n",
    "    \n",
    "mse = mean_squared_error(test_data_y, rounded_pred_y)\n",
    "print(f'Mean squared error: {mse:.2f}')\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(test_data_y)\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    if test_data_y[i] == rounded_pred_y[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "classification_accuracy = correct_predictions / total_predictions\n",
    "print(f'Classification accuracy: {classification_accuracy:.2f}')\n",
    "\n",
    "\n",
    "# print runtime\n",
    "print(f'Runtime: {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235dca8",
   "metadata": {},
   "source": [
    "## Implementing grid search over multiple hyperparameters with a random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0025325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'max_depth': 60, 'max_features': 'sqrt', 'n_estimators': 300}\n",
      "Mean squared error on training data:  0.9180986978517597\n",
      "Mean squared error: 1.00\n",
      "Runtime: 144.80 seconds\n"
     ]
    }
   ],
   "source": [
    "# define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [20, 30, 40, 50, 60],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# create the random forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# create the grid search object\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# fit the grid search object to the training data\n",
    "grid_search.fit(train_data_x, train_data_y)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# print the best hyperparameters and corresponding mean squared error\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Mean squared error on training data: \", abs(grid_search.best_score_))\n",
    "\n",
    "# evaluate the best model on the test data\n",
    "best_model = grid_search.best_estimator_\n",
    "pred_y = best_model.predict(test_data_x)\n",
    "\n",
    "# initializing a new array to add the rounded Hamming distances\n",
    "rounded_pred_y = np.array([])\n",
    "\n",
    "for prediction in pred_y:\n",
    "    \n",
    "    # round the predicted values to the nearest Hamming dist.\n",
    "    rounded_pred_y = np.append(rounded_pred_y, int(np.round(prediction)))\n",
    "    \n",
    "mse = mean_squared_error(test_data_y, rounded_pred_y)\n",
    "print(f'Mean squared error: {mse:.2f}')\n",
    "\n",
    "# print runtime\n",
    "print(f'Runtime: {end_time - start_time:.2f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0603c3f0",
   "metadata": {},
   "source": [
    "## Creating an ensemble of 3 random forests\n",
    "Originally, we had wanted to do grid search individually for each forest to test against the MLP NN; however, that was very computationally expensive and the run time was absurd. Instead of passing the parameters from the previous grid search as the parameters of each random forest in the ensemble directly, we chose to adjust the hyperparameters randomly by adjusting them by a positive or negative np.random.randint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d14db125",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting Model: 100%|██████████| 100/100 [04:25<00:00,  2.66s/iterations, Estimated remaining time =0m 23s]\n",
      "Fitting Model: 100%|██████████| 100/100 [02:33<00:00,  1.54s/iterations, Estimated remaining time =0m 40s] \n",
      "Fitting Model: 100%|██████████| 100/100 [04:08<00:00,  2.49s/iterations, Estimated remaining time =1m 3s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on test data: 0.97\n",
      "Classification accuracy: 0.36\n",
      "Runtime: 668.06 seconds\n"
     ]
    }
   ],
   "source": [
    "ensemble_size = 3\n",
    "\n",
    "# define the parameters for the random forest models\n",
    "params = {\n",
    "    #'n_estimators': 300,\n",
    "    'n_estimators' : grid_search.best_params_['n_estimators'],\n",
    "    #'max_depth': 50,\n",
    "    'max_depth' : grid_search.best_params_['max_depth'],\n",
    "    #'max_features': 'sqrt',\n",
    "    'max_features': grid_search.best_params_['max_features'],\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# create an ensemble of 3 random forest models\n",
    "models = []\n",
    "for i in range(ensemble_size):\n",
    "    rf = RandomForestRegressor(**params)\n",
    "    # each rf gets initialized with the same set of parameters but a different seed\n",
    "    rf.set_params(random_state=params['random_state'] + i)\n",
    "    \n",
    "    if i % 2 == 0:\n",
    "        rf.set_params(n_estimators = params['n_estimators'] + np.random.randint(10, high = 100))\n",
    "        rf.set_params(max_depth = params['max_depth'] - np.random.randint(1, high = 10))\n",
    "        models.append(rf)\n",
    "    else:\n",
    "        rf.set_params(n_estimators = params['n_estimators'] - np.random.randint(10, high = 100))\n",
    "        rf.set_params(max_depth = params['max_depth'] + np.random.randint(1, high = 10))\n",
    "        models.append(rf)\n",
    "    \n",
    "# fit the models to the training data\n",
    "for model in models:\n",
    "    # track progress of the training using tqdm\n",
    "    pbar = tqdm(range(100), desc='Fitting Model', unit='iterations')\n",
    "    for i in pbar:\n",
    "        model.fit(train_data_x, train_data_y)\n",
    "        if i % 10 == 0:\n",
    "            remaining_time = (time.time() - start_time) / (i + 1) * (100 - i - 1)\n",
    "            pbar.set_postfix({'Estimated remaining time ': f'{int(remaining_time // 60)}m {int(remaining_time % 60)}s'})\n",
    "            pbar.refresh()  # refresh the tqdm bar and show the updated postfix on a separate line\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# take the mean of the predictions from all models as the final prediction and round to nearest Hamming dist.\n",
    "#pred_y = sum(model.predict(test_data_x) for model in models) / len(models)\n",
    "#pred_y = round(pred_y)\n",
    "\n",
    "all_preds = []\n",
    "\n",
    "for model in models:\n",
    "    pred_y = model.predict(test_data_x)\n",
    "    all_preds.append(pred_y)\n",
    "    \n",
    "avg_pred_y = np.vstack(all_preds).mean(axis = 0)\n",
    "rounded_pred_y = np.round(avg_pred_y).astype(int)\n",
    "\n",
    "mse = mean_squared_error(test_data_y, rounded_pred_y)\n",
    "print(f'Mean squared error on test data: {mse:.2f}')\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = len(test_data_y)\n",
    "\n",
    "for i in range(total_predictions):\n",
    "    if test_data_y[i] == rounded_pred_y[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "classification_accuracy = correct_predictions / total_predictions\n",
    "print(f'Classification accuracy: {classification_accuracy:.2f}')\n",
    "\n",
    "# print runtime\n",
    "print(f'Runtime: {end_time - start_time:.2f} seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
