{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Minimum Hamming Distance Between Word and Vocabulary List\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Peter Barnett\n",
    "- William Lutz\n",
    "- Ricardo Sedano"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Our goal is to predict the minimum number of character substitutions required to turn a given input string of alphabetical characters into an English dictionary word (into *any* English dictionary word). That is, to predict the hamming distance between a given input string and its nearest English word. This problem can be solved with brute force nearest-neighbor search, or with a BK-tree, but takes a considerable amount of time if one needs to make this prediction many times. So, the purpose of this project is to be able to generate approximations of this shortest distance more quickly. The words will come from the open source WordSet dictionary, the training inputs will be generated from random strings, and the training labels will be generated using the brute force search approach. The same process is used to create the test data. We will train our model on this data and its performance will be measured by computing the average error |predicted distance - true distance| across the test dataset.\n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Hamming distance is the measured difference between two strings of equal lengths. As a simple example, the two strings 'car' and 'cat' would have a Hamming distance of 1, meaning that they only differ by one character. This mathematical concept was first published in 1950 his paper 'Error detecting error correcting codes' <a name = \"hamming\"></a>[<sup>[1]</sup>](#hammingnote). </br> </br>\n",
    "\n",
    "We see and acnowlegde how Hamming distance has multiple applications to solve difficult applications in science, for example, comparing genomic sequences <a name = \"pinheiro\"></a>[<sup>[2]</sup>](#pinheironote). Yet, while it has widely been used in DNA sequencing, error detection, and image processing, trying to find the minimum Hamming distance between an input and a large dataset is computationally expensive to calculate when iterating through many examples. \n",
    "\n",
    "For this project, our team aims to calculate the minimum Hamming distance from an input sting of text to dictionary entries of the same length. In this application, user creators can implement effecient algothitms for spell check, autocorrection<a name = \"lalwari\"></a>[<sup>[3]</sup>](#lalwarinote), text predition, and even word puzzle contruction. In the paper recently cited, the project team does not use Hamming distance for autocorrection. In a similar manner, they use the Levenshtein distance (a close relative to Hamming distance) in their project. This differs because as noted previously as well as in the recently sited paper, Hamming distance requires stings of the same length. That team concluded their algorithm is, in fact, suitable for autocorrection, specifically more so in web applications. Our project differs this by not honing on autocorrect, as well as not using the Levenshtein distance, but instead the orginal Hamming distance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Compared to a set of dictionary words, the process of calulating minimum Hamming distance creates solves a problem observed within the scope of our project. The problem this project is verifying distances between the trainig and test sets, while keeping a goal of observing the minimum Hamming distance output. \n",
    "\n",
    "The Hamming distance is a numberical value, and additionally when calulating error metrics like MSE we can get a better unsdertanding on how the data probelem's behavior. The problem is quantifiable and measurable, because as aformentioned, this project relies on mathematical processeses and the effectiveness of the algorithm can be observed through metrics like sentativity and F2 measure. Also from the data comprising of an established dictionary, the problem-solving/observation process is able to be replicated onto various other concepts Hamming distance highlightedi in the Background section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Detail how/where you obtained the data and cleaned it (if necessary)\n",
    "\n",
    "If the data cleaning process is very long (e.g., elaborate text processing) consider describing it briefly here in text, and moving the actual clearning process to another notebook in your repo (include a link here!).  The idea behind this approach: this is a report, and if you blow up the flow of the report to include a lot of code it makes it hard to read.\n",
    "\n",
    "Please give the following infomration for each dataset you are using\n",
    "- link/reference to obtain it\n",
    "- description of the size of the dataset (# of variables, # of observations)\n",
    "- what an observation consists of\n",
    "- what some critical variables are, how they are represented\n",
    "- any special handling, transformations, cleaning, etc you have done should be demonstrated here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "There are little to no obvious ethics & privacy concerns that arise from our project. The process highlighted in the project computes a numerical value (hamming distance), which is not involving other data, especially excluding personal data. Seeing as this process takes a string and compares to an established word, we could see a potential algorithm that creates text dialogue from these words. This connects to artificial intellegence; AI may be able to generate converstations with users online, which could involve ethics issues in spontaneous text production or privacy issues if AI creates upon platforms asking for personal information.\n",
    "\n",
    "The data used was not taken from human information, and omits collection bias by using random samples. Hamming distance is a numerical value, so there may not be a perspecive arguing the validity of a number. Additionally, there are arguably no harmful unintended use cases, as hamming distance itself cannot reveal the compared strings, and only returns a value. Because of the reasons aformentioned, our project passes the Deon ethics checklist. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"hammingnote\"></a>1.[^](#hamming): Hamming, R. W. (1950) Error detecting and error correcting codes. *Bell Systems Technical Journal, 29(2)*. https://ieeexplore.ieee.org/document/6772729<br> \n",
    " \n",
    "\n",
    "<a name=\"pinheironote\"></a>2.[^](#pinheiro): Pinheiro, H.P.  Analysis of Variance for Hamming Distances Applied to Unbalanced Designs. (https://ime.unicamp.br/sites/default/files/pesquisa/relatorios/rp-2001-30.pdf).<br>\n",
    "\n",
    "<a name=\"lalwaninote\"></a>3.[^](#lalwani): Lalwani, M. (2014) Efficient Algorithm for Auto Correction Using n-gram Indexing *Nirma University, Ahmedabad, India* (https://www.researchgate.net/profile/Mahesh-Lalwani/publication/266886742_Efficient_Algorithm_for_Auto_Correction_Using_n-gram_Indexing/links/547ee25e0cf2d2200edeaf9f/Efficient-Algorithm-for-Auto-Correction-Using-n-gram-Indexing.pdf).<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
